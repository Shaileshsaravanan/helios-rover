<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/style.css">
  </head>  
  <body>    
    <p id="status">Awaiting TF.js load</p>
    
    <video id="webcam" autoplay muted></video>
    
    <button id="enableCam">Enable Webcam</button>
    <button class="dataCollector" data-1hot="0" data-name="Class 1">Gather Class 1 Data</button>
    <button class="dataCollector" data-1hot="1" data-name="Class 2">Gather Class 2 Data</button>
    <button id="train">Train &amp; Predict!</button>
    <button id="reset">Reset</button>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.min.js" type="text/javascript"></script>
    <script type="module" src="/script.js"></script>
  </body>
</html>
<style>
    body {
    font-family: helvetica, arial, sans-serif;
    margin: 2em;
    }

    h1 {
    font-style: italic;
    color: #FF6F00;
    }


    video {
    clear: both;
    display: block;
    margin: 10px;
    background: #000000;
    width: 640px;
    height: 480px;
    }

    button {
    padding: 10px;
    float: left;
    margin: 5px 3px 5px 10px;
    }

    .removed {
    display: none;
    }

    #status {
    font-size:150%;
    }
</style>
<script>
const STATUS = document.getElementById('status');
const VIDEO = document.getElementById('webcam');
const ENABLE_CAM_BUTTON = document.getElementById('enableCam');
const RESET_BUTTON = document.getElementById('reset');
const TRAIN_BUTTON = document.getElementById('train');
const MOBILE_NET_INPUT_WIDTH = 224;
const MOBILE_NET_INPUT_HEIGHT = 224;
const STOP_DATA_GATHER = -1;
const CLASS_NAMES = [];

ENABLE_CAM_BUTTON.addEventListener('click', enableCam);
TRAIN_BUTTON.addEventListener('click', trainAndPredict);
RESET_BUTTON.addEventListener('click', reset);

let dataCollectorButtons = document.querySelectorAll('button.dataCollector');
for (let i = 0; i < dataCollectorButtons.length; i++) {
  dataCollectorButtons[i].addEventListener('mousedown', gatherDataForClass);
  dataCollectorButtons[i].addEventListener('mouseup', gatherDataForClass);
  CLASS_NAMES.push(dataCollectorButtons[i].getAttribute('data-name'));
}

let mobilenet = undefined;
let gatherDataState = STOP_DATA_GATHER;
let videoPlaying = false;
let trainingDataInputs = [];
let trainingDataOutputs = [];
let examplesCount = [];
let predict = false;

async function loadMobileNetFeatureModel() {
  const URL = 'https://tfhub.dev/google/tfjs-model/imagenet/mobilenet_v3_small_100_224/feature_vector/5/default/1';
  mobilenet = await tf.loadGraphModel(URL, {fromTFHub: true});
  STATUS.innerText = 'MobileNet v3 loaded successfully!';
    tf.tidy(function () {
    let answer = mobilenet.predict(tf.zeros([1, MOBILE_NET_INPUT_HEIGHT, MOBILE_NET_INPUT_WIDTH, 3]));
    console.log(answer.shape);  
  });
}

loadMobileNetFeatureModel();

async function enableCam() {
  if (videoPlaying) {
    STATUS.innerText = 'Webcam is already enabled.';
    return;
  }

  STATUS.innerText = 'Enabling webcam...';
  const stream = await navigator.mediaDevices.getUserMedia({ video: true });
  VIDEO.srcObject = stream;
  await new Promise((resolve) => VIDEO.onloadedmetadata = () => resolve());
  VIDEO.play();
  videoPlaying = true;
  STATUS.innerText = 'Webcam enabled.';
}

function gatherDataForClass(event) {
  if (gatherDataState === STOP_DATA_GATHER) return;
  const classId = parseInt(gatherDataState);
  if (event.type === 'mousedown') {
    gatherDataState = classId;
  } else if (event.type === 'mouseup') {
    gatherDataState = STOP_DATA_GATHER;
  }
  if (videoPlaying && gatherDataState !== STOP_DATA_GATHER) {
    gatherData();
  }
}

function gatherData() {
  if (!mobilenet) return;

  STATUS.innerText = 'Gathering data...';
  const videoFrame = VIDEO;
  const mobilenetInput = tf.browser.fromPixels(videoFrame).toFloat().resizeBilinear([MOBILE_NET_INPUT_WIDTH, MOBILE_NET_INPUT_HEIGHT]).expandDims();
  const features = mobilenet.predict(mobilenetInput).flatten();
  trainingDataInputs.push(features);
  trainingDataOutputs.push(gatherDataState);
  examplesCount[gatherDataState] = (examplesCount[gatherDataState] || 0) + 1;
  STATUS.innerText = `Gathering data for class ${CLASS_NAMES[gatherDataState]} (${examplesCount[gatherDataState]} examples)`;
}

async function trainAndPredict() {
  if (!mobilenet) {
    STATUS.innerText = 'Load the MobileNet model first!';
    return;
  }

  STATUS.innerText = 'Training the model...';
  const xs = tf.stack(trainingDataInputs);
  const ys = tf.oneHot(tf.tensor1d(trainingDataOutputs).toInt(), CLASS_NAMES.length);
  const model = tf.sequential();
  model.add(tf.layers.dense({ inputShape: [xs.shape[1]], units: CLASS_NAMES.length, activation: 'softmax' }));
  model.compile({ optimizer: 'adam', loss: 'categoricalCrossentropy', metrics: ['accuracy'] });
  await model.fit(xs, ys, { epochs: 10 });

  predict = true;
  STATUS.innerText = 'Model trained. Ready for predictions.';
  startPredicting(model);
}

async function startPredicting(model) {
  while (predict) {
    const videoFrame = VIDEO;
    const mobilenetInput = tf.browser.fromPixels(videoFrame).toFloat().resizeBilinear([MOBILE_NET_INPUT_WIDTH, MOBILE_NET_INPUT_HEIGHT]).expandDims();
    const features = mobilenet.predict(mobilenetInput).flatten();
    const predictions = model.predict(features.expandDims()).dataSync();
    const topPrediction = tf.argMax(predictions).dataSync()[0];
    STATUS.innerText = `Prediction: ${CLASS_NAMES[topPrediction]}`;
    await new Promise(requestAnimationFrame);
  }
}

function reset() {
  STATUS.innerText = 'Resetting...';
  trainingDataInputs = [];
  trainingDataOutputs = [];
  examplesCount = [];
  predict = false;
  gatherDataState = STOP_DATA_GATHER;
  STATUS.innerText = 'Reset complete. Collect new data.';
}
let model = tf.sequential();
model.add(tf.layers.dense({inputShape: [1024], units: 128, activation: 'relu'}));
model.add(tf.layers.dense({units: CLASS_NAMES.length, activation: 'softmax'}));

model.summary();

model.compile({
  optimizer: 'adam',
  loss: (CLASS_NAMES.length === 2) ? 'binaryCrossentropy': 'categoricalCrossentropy', 
  metrics: ['accuracy']  
});
function hasGetUserMedia() {
  return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
}

function enableCam() {
  if (hasGetUserMedia()) {
    const constraints = {
      video: true,
      width: 640, 
      height: 480 
    };

    navigator.mediaDevices.getUserMedia(constraints).then(function(stream) {
      VIDEO.srcObject = stream;
      VIDEO.addEventListener('loadeddata', function() {
        videoPlaying = true;
        ENABLE_CAM_BUTTON.classList.add('removed');
      });
    });
  } else {
    console.warn('getUserMedia() is not supported by your browser');
  }
}

 function gatherDataForClass() {
  let classNumber = parseInt(this.getAttribute('data-1hot'));
  gatherDataState = (gatherDataState === STOP_DATA_GATHER) ? classNumber : STOP_DATA_GATHER;
  dataGatherLoop();
}
function dataGatherLoop() {
  if (videoPlaying && gatherDataState !== STOP_DATA_GATHER) {
    let imageFeatures = tf.tidy(function() {
      let videoFrameAsTensor = tf.browser.fromPixels(VIDEO);
      let resizedTensorFrame = tf.image.resizeBilinear(videoFrameAsTensor, [MOBILE_NET_INPUT_HEIGHT, 
          MOBILE_NET_INPUT_WIDTH], true);
      let normalizedTensorFrame = resizedTensorFrame.div(255);
      return mobilenet.predict(normalizedTensorFrame.expandDims()).squeeze();
    });

    trainingDataInputs.push(imageFeatures);
    trainingDataOutputs.push(gatherDataState);
    
    // Intialize array index element if currently undefined.
    if (examplesCount[gatherDataState] === undefined) {
      examplesCount[gatherDataState] = 0;
    }
    examplesCount[gatherDataState]++;

    STATUS.innerText = '';
    for (let n = 0; n < CLASS_NAMES.length; n++) {
      STATUS.innerText += CLASS_NAMES[n] + ' data count: ' + examplesCount[n] + '. ';
    }
    window.requestAnimationFrame(dataGatherLoop);
  }
}
async function trainAndPredict() {
  predict = false;
  tf.util.shuffleCombo(trainingDataInputs, trainingDataOutputs);
  let outputsAsTensor = tf.tensor1d(trainingDataOutputs, 'int32');
  let oneHotOutputs = tf.oneHot(outputsAsTensor, CLASS_NAMES.length);
  let inputsAsTensor = tf.stack(trainingDataInputs);
  
  let results = await model.fit(inputsAsTensor, oneHotOutputs, {shuffle: true, batchSize: 5, epochs: 10, 
      callbacks: {onEpochEnd: logProgress} });
  
  outputsAsTensor.dispose();
  oneHotOutputs.dispose();
  inputsAsTensor.dispose();
  predict = true;
  predictLoop();
}

function logProgress(epoch, logs) {
  console.log('Data for epoch ' + epoch, logs);
}
function predictLoop() {
  if (predict) {
    tf.tidy(function() {
      let videoFrameAsTensor = tf.browser.fromPixels(VIDEO).div(255);
      let resizedTensorFrame = tf.image.resizeBilinear(videoFrameAsTensor,[MOBILE_NET_INPUT_HEIGHT, 
          MOBILE_NET_INPUT_WIDTH], true);

      let imageFeatures = mobilenet.predict(resizedTensorFrame.expandDims());
      let prediction = model.predict(imageFeatures).squeeze();
      let highestIndex = prediction.argMax().arraySync();
      let predictionArray = prediction.arraySync();

      STATUS.innerText = 'Prediction: ' + CLASS_NAMES[highestIndex] + ' with ' + Math.floor(predictionArray[highestIndex] * 100) + '% confidence';
    });

    window.requestAnimationFrame(predictLoop);
  }
}
/**
 * Purge data and start over. Note this does not dispose of the loaded 
 * MobileNet model and MLP head tensors as you will need to reuse 
 * them to train a new model.
 **/
 function reset() {
  predict = false;
  examplesCount.length = 0;
  for (let i = 0; i < trainingDataInputs.length; i++) {
    trainingDataInputs[i].dispose();
  }
  trainingDataInputs.length = 0;
  trainingDataOutputs.length = 0;
  STATUS.innerText = 'No data collected';
  
  console.log('Tensors in memory: ' + tf.memory().numTensors);
}
</script>